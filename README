README
======

I did it as a part of homework problem in the Advanced Machine Learning class taught by Prof Ji Liu in Fall 2014. 

This python codes show the use of Proximal Gradient Descent and Accelerated Proximal Gradient Descent algorithms
for solving LASSO formulation of optimization:

LASSO: \min_x f(x):= \frac{1}{2}|Ax-b|^2 + \lambda|x|_1
![equation](http://www.sciweavers.org/tex2img.php?eq=%5Cmin_x+f%28x%29%3A%3D+%5Cfrac%7B1%7D%7B2%7D%7CAx-b%7C%5E2+%2B+%5Clambda%7Cx%7C_1&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=)

LASSO formulation can reconstruct original data from its noisy version by using the sparsity constraint.

The current code takes a sparse vector (x*), applies a random linear transformation (i.e. multiply with a random matrix, A), and adds noise with it. It then takes the noisy vector and the transformation matrix and reconstracts the original sparse vector.

The accelerated version demonstrates that it converges much faster than the normal version.
